import os
import math
import h5py
import argparse
import numpy as np
from datasets import Dataset
import matplotlib.pylab as plt

def revert_zscore_to_uint8(normalized_array: np.ndarray) -> np.ndarray:
    """
    Converts a float32 array of z-scored spike counts back to a uint8 array.

    This function assumes the input float32 array was generated by z-scoring
    a uint8 array of threshold crossing counts on a per-channel (row) basis.
    The core assumption is that the original uint8 data consisted of integer
    counts (e.g., 0, 1, 2, ...), meaning the smallest difference between
    counts was 1.

    The function works by:
    1. For each channel, finding the unique normalized values.
    2. Calculating the differences between consecutive unique values to find the
       fundamental "step size". This step size in the normalized domain
       corresponds to a step of 1 in the original integer domain.
    3. Rescaling the normalized data by this step size to get centered integers.
    4. Shifting the centered integers so that the minimum value becomes 0.
    5. Clipping the result to the range [0, 20] and casting the type.

    Args:
        normalized_array: A 2D numpy array of shape (n_channels, n_timesteps) with dtype float32, representing the z-scored data.

    Returns:
        A 2D numpy array of the same shape with dtype uint8, representing the
        reconstructed original counts.
    """
    if not issubclass(normalized_array.dtype.type, np.floating):
        raise TypeError("Input array must be a floating-point type.")

    n_channels, _ = normalized_array.shape
    reconstructed_uint8_array = np.zeros_like(normalized_array, dtype=np.uint8)
    # A small tolerance for floating point comparisons
    TOLERANCE = 1e-6

    for i in range(n_channels):
        channel_data = normalized_array[i, :]
        unique_vals = np.unique(channel_data)

        # If a channel is constant (e.g., all zeros), it had no variance.
        # The original must also have been constant. We assume it was 0.
        if len(unique_vals) < 2:
            reconstructed_uint8_array[i, :] = 0
            continue

        # Find the step size in the normalized data. This corresponds to a
        # step of 1 in the original data (1 / standard_deviation).
        diffs = np.diff(np.sort(unique_vals))
        
        # Filter out negligible differences that might arise from precision errors.
        significant_diffs = diffs[diffs > TOLERANCE]

        if significant_diffs.size == 0:
            # This is an edge case where no clear step can be found, e.g., if
            # the unique values are [0, 5.0, 5.000001]. We assume this channel
            # cannot be reliably reconstructed and leave it as zeros.
            # A warning could be logged here in a real application.
            reconstructed_uint8_array[i, :] = 0
            continue
        
        # The smallest significant difference is our fundamental step size.
        step_size = np.min(significant_diffs)
        # Rescale the data by the step size. This undoes the standard deviation
        # scaling, resulting in values that are close to (original_data - mean).
        centered_integers = np.round(channel_data / step_size)

        # Shift the data so the minimum value is 0. This undoes the mean
        # subtraction, assuming the original counts started at 0.
        min_val = np.min(centered_integers)
        reconstructed_channel = centered_integers - min_val
        
        # Clip to the range [0, 20] and assign to the output array.
        reconstructed_uint8_array[i, :] = np.clip(reconstructed_channel, 0, 20).astype(np.uint8)
        
    return reconstructed_uint8_array


def find_hdf5_files(root_dir):
    """
    Crawls through a directory (including subdirectories), finds all files
    that end with ".mat" and returns the full paths of all the found files in a list.

    Args:
        root_dir: The root directory to start the search from.

    Returns:
        A list of full paths to the found .mat files, or an empty list if
        no files are found or if the root directory is invalid.
        Returns None if root_dir is not a valid directory.
    """

    if not os.path.isdir(root_dir):
        print(f"Error: '{root_dir}' is not a valid directory.")
        return None

    hdf5_files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for filename in filenames:
            if filename.endswith(".hdf5"):
                full_path = os.path.join(dirpath, filename)
                hdf5_files.append(full_path)
    return hdf5_files


def extract_session_id(file_path):
    """
    Extracts subject and session identifier strings from a full file path.

    Args:
        file_path (str): The full file path.

    Returns:
        str: Subject identifier string.
        str: Session identifier string.
    """
    directory, filename = os.path.split(file_path)
    subdirectory = os.path.basename(directory)
    filename_without_extension, _ = os.path.splitext(filename)
    return f"{subdirectory}_{filename_without_extension}"


def get_args_parser():
    parser = argparse.ArgumentParser('Consolidate data in multiple files into a single file', add_help=False)
    parser.add_argument('--data_dir',default="data",type=str, help='Data directory')
    parser.add_argument('--hf_repo_name',default="eminorhan/card",type=str, help='processed dataset will be pushed to this HF dataset repo')
    parser.add_argument('--token_count_limit',default=10_000_000, type=int, help='sessions with larger token counts than this will be split into chunks (default: 10_000_000)')
    return parser


if __name__ == '__main__':

    args = get_args_parser()
    args = args.parse_args()
    print(args)

    # get all .mat files in the sorted folder
    hdf5_files = find_hdf5_files(args.data_dir)
    print(f"Files: {hdf5_files}")
    print(f"Total number of files: {len(hdf5_files)}")

    # lists to store results for each session
    spike_counts_list, subject_list, session_list, segment_list = [], [], [], []

    # token counter
    n_tokens = 0

    for file_path in sorted(hdf5_files):
        print(f"Processing file: {file_path}")

        # subject, session identifiers
        session_id = extract_session_id(file_path)

        with h5py.File(file_path, "r") as f:
            spike_counts_tmp = []

            # iterate through the list of trial keys
            trial_keys = list(f.keys())
            for trial_key in trial_keys:
                # get the threshold crossing data (first 256 channels)
                data = f[trial_key]['input_features'][:][:, :256].T
                spike_counts = revert_zscore_to_uint8(data)
                spike_counts_tmp.append(spike_counts)

            # concatenate trials
            spike_counts = np.concatenate(spike_counts_tmp, axis=1)

            # token count of current session
            total_elements = np.prod(spike_counts.shape)

            # append sessions; if session data is large, divide spike_counts array into smaller chunks
            if total_elements > args.token_count_limit:
                n_channels, n_time_bins = spike_counts.shape
                num_segments = math.ceil(total_elements / args.token_count_limit)
                segment_size = math.ceil(n_time_bins / num_segments)
                print(f"Spike count dtype / shape / max: {spike_counts.dtype} / {spike_counts.shape} / {spike_counts.max()}. Dividing into {num_segments} smaller chunks ...")
                for i in range(num_segments):
                    start_index = i * segment_size
                    end_index = min((i + 1) * segment_size, n_time_bins)
                    sub_array = spike_counts[:, start_index:end_index]
                    spike_counts_list.append(sub_array)
                    subject_list.append("T15")
                    session_list.append(session_id)
                    segment_list.append(f"segment_{i}")
                    print(f"Divided into segment_{i} with shape / max: {sub_array.shape} / {sub_array.max()}")
                    n_tokens += np.prod(sub_array.shape)
            else:
                spike_counts_list.append(spike_counts)
                subject_list.append("T15")
                session_list.append(session_id)
                segment_list.append("segment_0")  # default segment id
                print(f"Spike count dtype / shape / max: {spike_counts.dtype} / {spike_counts.shape} / {spike_counts.max()} (segment_0)")
                n_tokens += np.prod(spike_counts.shape)

    def gen_data():
        for a, b, c, d in zip(spike_counts_list, subject_list, session_list, segment_list):
            yield {
                "spike_counts": a,
                "subject_id": b,
                "session_id": c,
                "segment_id": d
                }
            
    ds = Dataset.from_generator(gen_data, writer_batch_size=1)
    print(f"Number of tokens in dataset: {n_tokens} tokens")
    print(f"Number of rows in dataset: {len(ds)}")

    # push all data to hub 
    ds.push_to_hub(args.hf_repo_name, max_shard_size="1GB", token=True)